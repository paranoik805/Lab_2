Лабораторная работа 2.  
====

# 2.обучить нейронную сеть EfficientNet-B0 для решения задачи классификации изображений Oregon WildLife
* **Описание архитектуры:**   
 
* Размерность входного изображения (224x224x3): 
```
inputs = tf.keras.Input(shape=(RESIZE_TO, RESIZE_TO, 3))
```

* Слой 2D свертки. Параметры: 8 filtres, kernel_size=3 (матрица ядра = 3x3)
```
x = tf.keras.layers.Conv2D(filters=8, kernel_size=3)(inputs) //размер выходного тензора 222х222х8
```

* Операция подвыборки. Параметры по умолчанию: pool_size = 2х2, strides = 2.
```
x = tf.keras.layers.MaxPool2D()(x) //размер выходного тензора 111х111х8
```

* Преобразование многомерного тензора в одномерный. 
 ```
 x = tf.keras.layers.Flatten()(x) // размер выходного тензора 111x111x8 = 98568
 ```
 
 * Полносвязный Dense слой с 20 выходами и функцией активации softmax, которая определяет к какой категории и с какой вероятностью относится поданное на вход изображение.
```
outputs = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax)(x)
```

 ### Графики обучения для нейронной сети с одним сверточным слоем:
 
Синяя линия - на валидации  
Оранжевая линия - на обучении  

 ***График метрики точности:*** 
<img src="./epoch_categorical_accuracy v1.svg">

 ***График функции потерь:*** 
 
<img src="./epoch_loss v1.svg">

### Анализ результатов:

На графиках значение функции потерь на валидационном наборе данных выше, чем значение функции потерь на обучающем наборе данных, а значит наблюдается переобучение, что вызвано отсутствием нормальных условий и малым размером обучающей выборки (12000 изображение). 

# 3. С использованием техники обучения Transfer Learning. Обучить нейронную сеть EfficientNet-B0 (предобученную на базе изображений imagenet) для решения задачи классификации изображений Oregon WildLife

* **Описание структуры** 
* Добавленны 3 сверточных слоя Conv2D с изменнеными параметрами filters = 8 для первого слоя, и увеличение на 4 на последующих слоях, и kernel_size = 3 для первого слоя, и увеличение на 1 на последующих слоях.  
* Добавленны 3 слоя подвыборки MaxPool2D. Параметры по умолчанию: pool_size = 2х2, strides = 2.

 ```
  x = tf.keras.layers.Conv2D(filters=8, kernel_size=3)(inputs) //размером 222х222х8
  x = tf.keras.layers.MaxPool2D()(x) // размером 111x111x8 
  x = tf.keras.layers.Conv2D(filters=12, kernel_size=4)(x) //размером 108х108х12
  x = tf.keras.layers.MaxPool2D()(x) // размером 54x54x12
  x = tf.keras.layers.Conv2D(filters=16, kernel_size=5)(x) //размером 50х50х16
  x = tf.keras.layers.MaxPool2D()(x) // размером 25x25x16
  x = tf.keras.layers.Conv2D(filtres=24, kernel_size=6)(x) //размером 20х20х24
  x = tf.keras.layers.MaxPool2D()(x) // размером 10x10x24  
  x = tf.keras.layers.Flatten()(x) // размером 10x10x24 = 2400
  outputs = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax)(x)
  ```


 ### Графики обучения для нейронной сети с несколькими сверточными слоями:
 
Синяя линия - на валидации  
Оранжевая линия - на обучении  

 ***График метрики точности:*** 
<img src="./epoch_categorical_accuracy v2.svg">

 ***График функции потерь::*** 
 
<img src="./epoch_loss v2.svg">

### Анализ результатов:

На графиках значение функции потерь на валидационном наборе данных выше, чем значение функции потерь на обучающем наборе данных, а значит наблюдается переобучение. А также, из графика метрики точности видно, что значение точности у сверточной нейронной сети с одним слоем выше, чем у нейронной сети с 4 слоями. Однако видны все те же проблемы. Причинами этого могут быть, как и выше описанные предположения, так и внесенные изменения. 
